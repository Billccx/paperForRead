## 算法基础

### 卷积与全连接

卷积：局部链接、权值共享、保持空间结构

### 卷积网络各层感受野大小

感受野递推式：
$$
R_e^{(i)}=\min \left(R_e^{(i-1)}+\left(kernalsize^{(i)}-1\right) \prod_{j=0}^{i-1} stride^{(j)}, \quad L_e\right)
$$
Le是原始图像的尺寸



### 卷积变种

#### 分组卷积

<img src="D:\paper\paperForRead\images\conv1.png" style="zoom:67%;" />

将输入和输出通道划分为相同的组，卷积运算仅在对应的组中进行

分组卷积优点：如果分g组，则在输入输出通道数不变的情况下，参数量缩小为原先的1/g

当g=cin=cout时，为特殊情况深度可分离卷积，输入和输出的每一个通道一一对应，因此后面常常跟进1*1卷积融合通道之间的信息



#### 转置卷积

亦称反卷积，可以逆转普通卷积的维度变化，但并不能保证值可逆

一个卷积核尺寸为kw*kh 、滑动步长为(sw,sh)、边界填充尺寸为(pw,ph)的普通卷积，其所对应的转置卷积为：

对输入特征图的相邻像素之间填充(sw-1,sh-1)个0

输入特征图左右padding kw-pw-1个0，上下kh-ph-1

然后卷积核尺寸为kw*kh，滑动步长1进行卷积

**用途**：对特征图进行扩张或上采样

<img src="D:\paper\paperForRead\images\deconv.png" style="zoom:60%;" />

最右侧为反卷积对应的卷积实现操作步骤



#### 空洞卷积

一般采用池化操作（pooling）来扩大特征图的感受野，但这同时会降低特征图的分辨率，丢失一些信息（如内部数据结构、空间层级信息等），导致后续的上采样操作（如转置卷积）无法还原一些细节

因此引入空洞卷积，不通过池化等下采样操作扩大感受野，而是在标准的卷积核中注入“空洞”，以增加卷积核的感受野。

<img src="D:\paper\paperForRead\images\hollow.png" style="zoom:60%;" />

黄色部分是方块中心元素可以感受到的范围，颜色越深表示感受的次数/程度越多



#### 可变形卷积

卷积核不再是规则形状的，引入一组可学习的偏置参数决定卷积核的形状

<img src="D:\paper\paperForRead\images\conv2.png" style="zoom:60%;" />

可形变卷积的感受野不再是规则的方形区域





### 梯度消失与梯度爆炸

#### 梯度消失

<img src="D:\paper\paperForRead\images\vanish.png" style="zoom:60%;" />

由于sigmoid函数的导数不超过0.25，因此网络变深后梯度消失，浅层难以训练

**解决方案**：Relu，导数为1

#### 梯度爆炸

权重初始化过大



### ResNet

网络退化问题：随着网络深度增加，网络的表现先是逐渐增加至饱和，然后迅速下降。

网络退化不是过拟合导致的，过拟合的表现是训练集误差小，而测试集误差大，而网络退化不论训练集还是测试集，效果都不如浅层网络好

按照常理来说，深层网络不应该表现得更差，那么可以构造一个更深的网络，其最后几层仅是网络f第k层输出的恒等映射，就可以取得与f一致的结果，一个合理的猜测就是，恒等映射并不是那么好学的。

![](D:\paper\paperForRead\images\resnet.png)

极端情况下，将残差映射推向0映射，仅保留近道的效果



### 瓶颈层

Bottleneck layer

瓶颈结构指的是通道数的变化，即原通道数先减少，后增大到原来的大小

#### resnet中的瓶颈结构

<img src="D:\paper\paperForRead\images\bottle.png" style="zoom:67%;" />





### 交叉验证

为了解决简单交叉验证的不足，提出k-fold交叉验证：

1、首先，将全部样本划分成k个大小相等的样本子集；2、依次遍历这k个子集，每次把当前子集作为验证集，其余所有样本作为训练集，进行模型的训练和评估；3、最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k通常取10.





### Batch Normalization

[【神经网络】网络中BN层的作用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/513326484)

[Batch Norm详解之原理及为什么神经网络需要它 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/441573901)

BN层的结构

<img src="D:\paper\paperForRead\images\bn3.png" style="zoom:60%;" />

可训练参数$\beta$,$\gamma$,各参数的向量形状如下图所示：

<img src="D:\paper\paperForRead\images\bn5.png" style="zoom:60%;" />



计算过程：

![](D:\paper\paperForRead\images\bn4.png)

在训练过程中会以增量更新的方式记录均值和方差，在推理阶段，由于一次只有一个样本，没有批的概念，故会将让些在训练过程中存储下来的参数参与计算

![](D:\paper\paperForRead\images\bn6.png)





BN层通常放在前一层的激活之后

![](D:\paper\paperForRead\images\bn1.png)

作用有几点：

1、各特征的分布差异过大，导致网络难以训练，BN层分通道进行正则化（图像的特征就是通道？），就相当于对各特征进行正则化

2、

